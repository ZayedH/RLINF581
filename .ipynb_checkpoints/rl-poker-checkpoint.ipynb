{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './utils')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's play one against the other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = [\"Ace\", \"King\", \"Queen\", \"Jack\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]\n",
    "signs = [\"Club\", \"Diamond\", \"Heart\", \"Spade\"]\n",
    "initialW = 100\n",
    "n = 7\n",
    "m = 11\n",
    "it = int(5*1e3)\n",
    "lr = .5\n",
    "exp = .4\n",
    "alpha = 3.\n",
    "decay = 1.\n",
    "maxBet = 10\n",
    "retreat = 1\n",
    "parameters = [[1, 1] for i in range(maxBet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cards: \n",
    "    \n",
    "    def __init__(self, firstPlayer, secondPlayer):\n",
    "        self.cards = 4*list(range(n, m+1))\n",
    "        self.firstPlayer = firstPlayer\n",
    "        self.secondPlayer = secondPlayer\n",
    "        self.turn = random.randint(0, 1)\n",
    "        \n",
    "    def deal(self):\n",
    "        random.shuffle(self.cards)\n",
    "        self.turn = 1 - self.turn\n",
    "        self.firstPlayer.hand = [self.cards[0], self.cards[2]]\n",
    "        self.firstPlayer.sum = sum(self.firstPlayer.hand)\n",
    "        self.secondPlayer.hand = [self.cards[1], self.cards[3]]\n",
    "        self.secondPlayer.sum = sum(self.secondPlayer.hand)\n",
    "        \n",
    "    def restart(self):\n",
    "        self.secondPlayer.reset()\n",
    "        self.firstPlayer.reset()\n",
    "        self.turn = random.randint(0, 1)\n",
    "        \n",
    "    def reward(self, first, second, trainable):\n",
    "        first.update(second, trainable)\n",
    "        second.update(first, trainable)\n",
    "            \n",
    "    def play(self, first, second, trainable):\n",
    "        \n",
    "        first.lastAction = None\n",
    "        actions = list(range(1, min(maxBet, first.worth, second.worth)+1))\n",
    "        first.action = first.chooseAction(actions, None)    \n",
    "        \n",
    "        second.lastAction = None\n",
    "        actions = list(range(first.action, min(maxBet, first.worth, second.worth)+1))\n",
    "        actions.append(0)\n",
    "        second.action = second.chooseAction(actions, first.action)\n",
    "\n",
    "        while first.action*second.action > 0 and first.action != second.action:\n",
    "            \n",
    "            first.lastAction = first.action\n",
    "            actions = list(range(second.action , min(maxBet, first.worth, second.worth)+1))\n",
    "            actions.append(0)\n",
    "            first.action = first.chooseAction(actions, second.action)\n",
    "            \n",
    "            if first.action > 0 and second.action != first.action:\n",
    "                \n",
    "                second.lastAction = second.action\n",
    "                actions = list(range(first.action, min(maxBet, first.worth, second.worth)+1))\n",
    "                actions.append(0)\n",
    "                second.action = second.chooseAction(actions, first.action)\n",
    "\n",
    "        self.reward(first, second, trainable)\n",
    "\n",
    "    def train(self, rounds = 10, trainable = True):\n",
    "\n",
    "        score = 0\n",
    "        for _ in range(rounds):\n",
    "            self.restart()\n",
    "            while self.firstPlayer.worth * self.secondPlayer.worth > 0:\n",
    "                self.deal()\n",
    "                if self.turn:\n",
    "                    self.play(self.firstPlayer, self.secondPlayer, trainable)\n",
    "                else:\n",
    "                    self.play(self.secondPlayer, self.firstPlayer, trainable)\n",
    "            score += (self.firstPlayer.worth > 0)\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def human(self, results = True, trainable = False):\n",
    "\n",
    "        self.restart()\n",
    "        while self.firstPlayer.worth * self.secondPlayer.worth > 0:\n",
    "            self.deal()\n",
    "            if self.turn:\n",
    "                self.play(self.firstPlayer, self.secondPlayer, trainable)\n",
    "            else:\n",
    "                self.play(self.secondPlayer, self.firstPlayer, trainable)\n",
    "            \n",
    "            if results:\n",
    "                print(\"-------------------------\")\n",
    "                print(\"The {0} hand and last action were: {1}-{2}\".format(self.secondPlayer.name, self.secondPlayer.hand, self.secondPlayer.action))\n",
    "                print(\"-------------------------\")\n",
    "                print(\"The {0} hand and last action were: {1}-{2}\".format(self.firstPlayer.name, self.firstPlayer.hand, self.firstPlayer.action))\n",
    "                print(\"-------------------------\")\n",
    "                print(\"Current Net Worth of {0}: {1} - Current Net Worth of {2}: {3}\".format(self.firstPlayer.name, self.firstPlayer.worth, self.secondPlayer.name, self.secondPlayer.worth))\n",
    "                print(\"=========================\")\n",
    "            \n",
    "        if self.secondPlayer.worth > 0:\n",
    "            print(\"\\n And the final winner is {0}! :)\".format(self.secondPlayer.name))\n",
    "        else:\n",
    "            print(\"\\n And the final winner is {0}! :>\".format(self.firstPlayer.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    \n",
    "    def __init__(self, name = \"Player\", lr = lr, exp = exp):\n",
    "        self.worth = initialW\n",
    "        self.statDict = {}\n",
    "        self.lr = lr\n",
    "        self.exp = exp\n",
    "        self.name = name\n",
    "        \n",
    "    def reset(self):\n",
    "        self.worth = initialW\n",
    "        \n",
    "    def chooseAction(self, actions, advAction):\n",
    "        \n",
    "        if advAction is None:\n",
    "            return random.choice(actions)\n",
    "        \n",
    "        elif random.random() < exp:\n",
    "            return random.choice(actions)\n",
    "\n",
    "        action = None\n",
    "        vmax = - 1e10\n",
    "        state = str(self.sum) + \"-\" + str(advAction)\n",
    "        for act in actions:\n",
    "            current = 0 if self.statDict.get(state) is None or self.statDict.get(state).get(str(act)) is None else self.statDict.get(state).get(str(act))\n",
    "            if vmax < current:\n",
    "                vmax = current\n",
    "                action = act\n",
    "                    \n",
    "            return action\n",
    "        \n",
    "    def update(self, adverser, trainable):\n",
    "        if trainable:\n",
    "            if adverser.action > 0:\n",
    "                if self.lastAction is None:\n",
    "                    state = str(self.sum) + \"-\" + str(adverser.action)\n",
    "                else:\n",
    "                    state = str(self.sum) + \"-\" + str(adverser.action) + \"-\" + str(self.lastAction)\n",
    "                    \n",
    "                if self.statDict.get(state) is None:\n",
    "                    self.statDict[state] = {}\n",
    "                if self.statDict.get(state).get(str(self.action)) is None:\n",
    "                    self.statDict[state][str(self.action)] = 0\n",
    "\n",
    "                if self.action == adverser.action:\n",
    "                    if self.sum > adverser.sum:\n",
    "                        self.statDict[state][str(self.action)] += self.lr*(  self.action - self.statDict[state][str(self.action)])\n",
    "                    elif self.sum < adverser.sum:\n",
    "                        self.worth -= self.action\n",
    "                        self.statDict[state][str(self.action)] += self.lr*(- self.action - self.statDict[state][str(self.action)])\n",
    "\n",
    "                elif self.action == 0:\n",
    "                    if self.lastAction is None:\n",
    "                        self.worth -= retreat\n",
    "                        self.statDict[state][str(self.action)] += self.lr*(- retreat - self.statDict[state][str(self.action)])\n",
    "\n",
    "                    else:\n",
    "                        self.worth -= self.lastAction\n",
    "                        self.statDict[state][str(self.action)] += self.lr*(- self.lastAction - self.statDict[state][str(self.action)])\n",
    "        \n",
    "        else:\n",
    "            if self.action == adverser.action and self.sum < adverser.sum:\n",
    "                self.worth -= self.action\n",
    "            elif self.action == 0:\n",
    "                if self.lastAction is None:\n",
    "                    self.worth -= retreat\n",
    "                else:\n",
    "                    self.worth -= self.lastAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    \n",
    "    def __init__(self, name = \"HumanPlayer\"):\n",
    "        self.name = name\n",
    "        \n",
    "    def chooseAction(self, actions, advAction):\n",
    "        print(\"State: \", self.hand)\n",
    "        \n",
    "        if advAction is None: pass\n",
    "        else:\n",
    "            print(\"Adverser played: \", advAction)\n",
    "            \n",
    "        action = int(input(\"Input your action: \")) \n",
    "        \n",
    "        while action not in actions:\n",
    "            print(\"Try a number in \", actions)\n",
    "            action = int(input(\"Input your action: \"))  \n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def update(self, adverser, trainable):\n",
    "        if self.action == adverser.action and self.sum < adverser.sum:\n",
    "            self.worth -= self.action\n",
    "        elif self.action == 0:\n",
    "            if self.lastAction is None:\n",
    "                self.worth -= retreat\n",
    "            else:\n",
    "                self.worth -= self.lastAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO TRAIN A BASIC SIMPLE MODEL YOU CAN USE, IT IS NOT VERY SMART\n",
    "# firstPlayer = Player()\n",
    "# secondPlayer = Player()\n",
    "# cards = Cards(firstPlayer, secondPlayer)\n",
    "# cards.train(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TO PLAY AGAINST A VIRTUAL PLAYER, YOU CAN UNCOMMENT THE FOLLOWING LINES AFTER TRAINING THE AGENT\n",
    "# firstPlayer.reset()\n",
    "# human = HumanPlayer()\n",
    "# cards = Cards(firstPlayer, human)\n",
    "# cards.human()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyPlayer(Player):\n",
    "    \n",
    "    def __init__(self, name = \"GreedyPlayer\"):\n",
    "        self.worth = initialW\n",
    "        self.name = name\n",
    "        \n",
    "    def chooseAction(self, actions, advAction):\n",
    "        if advAction is None:\n",
    "            return random.choice(actions)\n",
    "        if self.sum > m + n:\n",
    "            return advAction\n",
    "        return 0\n",
    "    \n",
    "    def update(self, adverser, trainable):\n",
    "        if self.action == adverser.action and self.sum < adverser.sum:\n",
    "            self.worth -= self.action\n",
    "        elif self.action == 0:\n",
    "            if self.lastAction is None:\n",
    "                self.worth -= retreat\n",
    "            else:\n",
    "                self.worth -= self.lastAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO TRAIN A BASIC SIMPLE MODEL YOU CAN USE, IT IS NOT VERY SMART\n",
    "# firstPlayer = GreedyPlayer()\n",
    "# secondPlayer = Player()\n",
    "# cards = Cards(firstPlayer, secondPlayer)\n",
    "# cards.train(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TO PLAY AGAINST A VIRTUAL PLAYER, YOU CAN UNCOMMENT THE FOLLOWING LINES AFTER TRAINING THE AGENT\n",
    "# human = HumanPlayer()\n",
    "# cards = Cards(firstPlayer, human)\n",
    "# cards.human()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling(Player):\n",
    "    def __init__(self, parameters = parameters, name = \"Thompson\", exp = exp):\n",
    "        self.exp = exp\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        self.nbDraws = np.zeros((2*(m-n) + 1, maxBet + 1))\n",
    "        self.cumRewards = np.zeros((2*(m-n) + 1, maxBet + 1))\n",
    "\n",
    "    def chooseAction(self, actions, advAction):\n",
    "        if advAction is None:\n",
    "            return random.choice(actions)\n",
    "        \n",
    "        elif random.random() < self.exp:\n",
    "            return random.choice(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        hand, action = state.split(\"-\")\n",
    "        cards = [int(card) for card in hand[1:-1].split(\",\")]\n",
    "        action = int(action)\n",
    "        \n",
    "        estimated = random.betavariate(self.parameters[action-1][0], self.parameters[action-1][1])\n",
    "        estimated = 2*n + int(2*(m-n)*estimated)\n",
    "    \n",
    "        if sum(cards) >= estimated: \n",
    "            return int(action)\n",
    "        return 0 \n",
    "        \"\"\"\n",
    "        action = advAction\n",
    "        \n",
    "        expression = np.zeros(maxBet + 1)\n",
    "        for i in range(len(actions)):\n",
    "            act = actions[i]\n",
    "            expression[act] = np.random.beta(self.parameters[action - 1][0] + self.cumRewards[self.sum-2*n, act],\n",
    "                                             self.parameters[action - 1][1] + self.nbDraws[self.sum-2*n, act] - self.cumRewards[self.sum-2*n, act])\n",
    "        return randmax(expression)\n",
    "        \n",
    "    def update(self, adverser, trainable):\n",
    "        if trainable:\n",
    "            if self.action == adverser.action:\n",
    "                if self.sum > adverser.sum:\n",
    "                    self.parameters[adverser.action - 1][0] += 0.5\n",
    "                elif self.sum < adverser.sum:\n",
    "                    self.worth -= adverser.action\n",
    "                    self.parameters[adverser.action - 1][1] += 0.5\n",
    "                    \n",
    "            elif self.action == 0:\n",
    "                if self.lastAction is None:\n",
    "                    self.worth -= retreat\n",
    "                else:\n",
    "                    self.worth -= self.lastAction\n",
    "                self.parameters[adverser.action - 1][1] += 0.5\n",
    "            \n",
    "        else:\n",
    "            if self.action == adverser.action and self.sum < adverser.sum:\n",
    "                self.worth -= self.action\n",
    "            elif self.action == 0:\n",
    "                if self.lastAction is None:\n",
    "                    self.worth -= retreat\n",
    "                else:\n",
    "                    self.worth -= self.lastAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstPlayer = ThompsonSampling()\n",
    "# secondPlayer = Player()\n",
    "# cards = Cards(firstPlayer, secondPlayer)\n",
    "# cards.train(it, bandit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cards = Cards(firstPlayer, HumanPlayer())\n",
    "# cards.human(human = True, bandit = True, result = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCB Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(Player):\n",
    "    def __init__(self, alpha = alpha, name = \"UCB\", exp = exp):\n",
    "        self.exp = exp\n",
    "        self.name = name\n",
    "        self.alpha = alpha\n",
    "        self.nbDraws = np.zeros((2*(m-n) + 1, maxBet + 1))\n",
    "        self.cumRewards = np.zeros((2*(m-n) + 1, maxBet + 1))\n",
    "\n",
    "    def chooseAction(self, actions, advAction):\n",
    "        \n",
    "        if random.random() < self.exp:\n",
    "            final = random.choice(actions)\n",
    "            self.cumRewards[self.sum-2*n, final] += 1\n",
    "            return final\n",
    "        \n",
    "        expression = np.zeros(maxBet + 1)\n",
    "        calls = np.sum(self.nbDraws)\n",
    "        \n",
    "        for i in range(len(actions)):\n",
    "            act = actions[i]\n",
    "            if self.nbDraws[self.sum-2*n, act] < 1: expression[act] = np.inf\n",
    "            else:\n",
    "                expression[act] = self.cumRewards[self.sum-2*n, act]/self.nbDraws[self.sum-2*n, act] + np.sqrt((self.alpha*np.log(calls+1))/self.nbDraws[self.sum-2*n, act])\n",
    "        \n",
    "        final = randmax(expression)\n",
    "        self.cumRewards[self.sum-2*n, final] += 1\n",
    "        return final\n",
    "        \n",
    "    def update(self, adverser, trainable): \n",
    "        if trainable:\n",
    "            if self.action == adverser.action:\n",
    "                if self.sum > adverser.sum:\n",
    "                    self.cumRewards[self.sum-2*n, adverser.action] += adverser.action\n",
    "                elif self.sum < adverser.sum:\n",
    "                    self.worth -= adverser.action\n",
    "                    self.cumRewards[self.sum-2*n, adverser.action] -= adverser.action\n",
    "            elif self.action == 0:\n",
    "                if self.lastAction is None:\n",
    "                    self.worth -= retreat\n",
    "                else:\n",
    "                    self.worth -= self.lastAction\n",
    "                self.cumRewards[self.sum-2*n, adverser.action] -= adverser.action\n",
    "        else:\n",
    "            if self.action == adverser.action and self.sum < adverser.sum:\n",
    "                self.worth -= adverser.action\n",
    "            elif self.action == 0:\n",
    "                if self.lastAction is None:\n",
    "                    self.worth -= retreat\n",
    "                else:\n",
    "                    self.worth -= self.lastAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstPlayer = UCB()\n",
    "# secondPlayer = Player()\n",
    "# cards = Cards(firstPlayer, secondPlayer)\n",
    "# cards.train(it, bandit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cards = Cards(firstPlayer, HumanPlayer())\n",
    "# cards.human(human = True, bandit = True, result = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who's better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2537"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstPlayer = Player(lr = .1, exp = .1)\n",
    "secondPlayer = Player(lr = .9, exp = 0)\n",
    "cards = Cards(firstPlayer, secondPlayer)\n",
    "cards.train(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1402"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedyPlayer = GreedyPlayer()\n",
    "cards = Cards(firstPlayer, greedyPlayer)\n",
    "cards.train(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2766"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UCBPlayer = UCB()\n",
    "cards = Cards(firstPlayer, UCBPlayer)\n",
    "cards.train(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThompsonPlayer = ThompsonSampling()\n",
    "cards = Cards(firstPlayer, ThompsonPlayer)\n",
    "cards.train(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3433"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards = Cards(greedyPlayer, ThompsonPlayer)\n",
    "cards.train(it, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3420"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards = Cards(greedyPlayer, UCBPlayer)\n",
    "cards.train(it, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2459"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards = Cards(UCBPlayer, ThompsonPlayer)\n",
    "cards.train(it, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2538"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cards = Cards(ThompsonPlayer, UCBPlayer)\n",
    "cards.train(it, trainable = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who's better Zayed or the computer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cards = Cards(GreedyPlayer(), HumanPlayer())\n",
    "cards.human(results = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
